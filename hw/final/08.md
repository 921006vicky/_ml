copy范揚玄

原理說明
自動微分（Autograd）：PyTorch 自動追蹤變數之間的運算並自動產生對應的導數（gradient）。

梯度下降法（Gradient Descent）：每次疊代時，從目前位置沿著函數的負梯度方向更新變數，逐步逼近最小值。

歸零梯度：在每次疊代後必須將梯度清空，否則 PyTorch 會將梯度累加，導致錯誤的更新方向