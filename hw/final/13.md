使用chatgpt製作
# Gradient Boosting 範例與原理說明（未上課教過）

## ✅ 使用方法：Gradient Boosting Classifier

Gradient Boosting 是一種集成學習方法，不同於我們上課教過的 KNN、SVM、決策樹、隨機森林等方法。它是一種**逐步優化的弱學習器加法模型**。

---

## 🔬 原理簡介：Gradient Boosting 是什麼？

Gradient Boosting（梯度提升）結合了多個弱模型（通常是小型決策樹），透過一個接一個的方式訓練：

1. 第一個模型先對訓練資料做初步預測。
2. 接著第二個模型針對第一個模型的誤差進行學習與修正。
3. 如此重複，後續每一個模型都學習「上一個模型錯的地方」。
4. 最後將所有模型的預測加總，得到最終結果。

> 簡單來說：每一個模型都是在**修正上一個模型做錯的部分**。

---

## 📘 特性與優點

- 強大的泛化能力（比單一決策樹好很多）
- 適合處理高維特徵資料
- 學習速度快且效果好（在 Kaggle 比賽中非常常見）
- 可應用於分類、回歸、排序等問題

---

| 欄位名稱               | 說明                                                          |
| ------------------ | ----------------------------------------------------------- |
| **precision**（精確率） | 被預測為某類別中，實際正確的比例（少錯抓）<br>→ `TP / (TP + FP)`                 |
| **recall**（召回率）    | 所有正確標記中，有多少被正確抓出來（少漏抓）<br>→ `TP / (TP + FN)`                |
| **f1-score**       | Precision 與 Recall 的加權平均（平衡兩者）<br>→ `2 * (P * R) / (P + R)` |
| **support**        | 該類別在測試集中實際的樣本數                                              |

| 類別  | 說明       | 效果                |
| --- | -------- | ----------------- |
| `0` | 代表「良性腫瘤」 | F1 = 0.94，分類表現良好  |
| `1` | 代表「惡性腫瘤」 | F1 = 0.97，分類效果非常好 |
